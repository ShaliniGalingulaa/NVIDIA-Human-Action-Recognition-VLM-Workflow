# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cm_-TkI_iREvg6kRnrboU8a1K-BRvA1w
"""

import gradio as gr
import torch
import numpy as np
import cv2
from transformers import AutoProcessor, AutoModelForVideoClassification
from moviepy.editor import VideoFileClip

class HumanActionRecognitionWorkflow:
    def __init__(self):
        # Load pre-trained VLM model
        self.model_name = "nvidia/tao-action-recognition"
        self.processor = AutoProcessor.from_pretrained(self.model_name)
        self.model = AutoModelForVideoClassification.from_pretrained(self.model_name)

        # Action categories (can be expanded)
        self.actions = [
            "walking", "running", "jumping", "dancing",
            "sitting", "standing", "climbing", "swimming"
        ]

    def trim_video(self, video_path, duration=20):
        """Trim video to specified duration."""
        clip = VideoFileClip(video_path)
        trimmed_clip = clip.subclip(0, min(duration, clip.duration))
        trimmed_path = f"trimmed_{video_path}"
        trimmed_clip.write_videofile(trimmed_path)
        return trimmed_path

    def preprocess_video(self, video_path):
        """Preprocess video for VLM input."""
        video = cv2.VideoCapture(video_path)
        frames = []
        while video.isOpened():
            ret, frame = video.read()
            if not ret:
                break
            frames.append(frame)
        video.release()

        # Uniformly sample frames
        sampled_frames = frames[::len(frames)//16][:16]
        inputs = self.processor(
            videos=sampled_frames,
            return_tensors="pt"
        )
        return inputs

    def recognize_action(self, video_path, target_action):
        """Recognize specific human action in video."""
        inputs = self.preprocess_video(video_path)

        with torch.no_grad():
            outputs = self.model(**inputs)

        # Get prediction probabilities
        probs = torch.softmax(outputs.logits, dim=-1)[0]
        top_actions = torch.topk(probs, k=3)

        # Compute success rate for target action
        action_index = self.actions.index(target_action)
        success_rate = probs[action_index].item() * 100

        return {
            "success_rate": success_rate,
            "top_actions": [self.actions[idx] for idx in top_actions.indices[0]]
        }

    def compare_videos(self, video1_path, video2_path, target_action):
        """Compare action recognition between two videos."""
        # Trim videos to same length
        video1_trimmed = self.trim_video(video1_path)
        video2_trimmed = self.trim_video(video2_path)

        result1 = self.recognize_action(video1_trimmed, target_action)
        result2 = self.recognize_action(video2_trimmed, target_action)

        return result1, result2

def create_gradio_interface(workflow):
    def recognize_and_compare(video1, video2, target_action):
        try:
            result1, result2 = workflow.compare_videos(video1, video2, target_action)
            return (
                f"Video 1 Success Rate: {result1['success_rate']:.2f}%\n"
                f"Video 1 Top Actions: {', '.join(result1['top_actions'])}\n\n"
                f"Video 2 Success Rate: {result2['success_rate']:.2f}%\n"
                f"Video 2 Top Actions: {', '.join(result2['top_actions'])}"
            )
        except Exception as e:
            return f"Error processing videos: {str(e)}"

    iface = gr.Interface(
        fn=recognize_and_compare,
        inputs=[
            gr.Video(label="Video 1"),
            gr.Video(label="Video 2"),
            gr.Dropdown(
                choices=["walking", "running", "jumping", "dancing",
                         "sitting", "standing", "climbing", "swimming"],
                label="Target Action"
            )
        ],
        outputs=gr.Textbox(label="Recognition Results"),
        title="NVIDIA Human Action Recognition",
        description="Compare action recognition across two videos"
    )
    return iface

def main():
    workflow = HumanActionRecognitionWorkflow()
    interface = create_gradio_interface(workflow)
    interface.launch(share=True)

if __name__ == "__main__":
    main()