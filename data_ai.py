# -*- coding: utf-8 -*-
"""data_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12tWrQfUgkAYhSQG7WgjvKkluYfJh28Bt
"""

!pip install transformers torchvision datasets

import torch
import torch.nn as nn
import torchvision.models as models
from transformers import BertModel, BertTokenizer

class VLM(nn.Module):
    def __init__(self, image_embedding_dim=512, text_embedding_dim=768, projection_dim=256):
        super(VLM, self).__init__()

        # Vision Encoder: Pretrained ResNet
        resnet = models.resnet50(pretrained=True)
        self.vision_encoder = nn.Sequential(*list(resnet.children())[:-1])  # Remove the classification head
        self.vision_fc = nn.Linear(resnet.fc.in_features, image_embedding_dim)

        # Text Encoder: Pretrained BERT
        self.text_encoder = BertModel.from_pretrained("bert-base-uncased")
        self.text_fc = nn.Linear(text_embedding_dim, image_embedding_dim)

        # Shared projection layer for image and text
        self.projection = nn.Linear(image_embedding_dim, projection_dim)

    def forward(self, images, text_tokens):
        # Vision pathway
        vision_features = self.vision_encoder(images).squeeze(-1).squeeze(-1)  # Global pooling
        vision_features = self.vision_fc(vision_features)
        vision_features = self.projection(vision_features)

        # Text pathway
        text_features = self.text_encoder(**text_tokens).pooler_output
        text_features = self.text_fc(text_features)
        text_features = self.projection(text_features)

        return vision_features, text_features

from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os

class CustomDataset(Dataset):
    def __init__(self, image_dir, captions, tokenizer, image_size=224):
        self.image_dir = image_dir
        self.captions = captions
        self.tokenizer = tokenizer
        self.image_size = image_size
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

    def __len__(self):
        return len(self.captions)

    def __getitem__(self, idx):
        image_path = os.path.join(self.image_dir, f"{idx}.jpg")
        image = Image.open(image_path).convert("RGB")
        image = self.transform(image)

        caption = self.captions[idx]
        text_tokens = self.tokenizer(caption, return_tensors="pt", padding=True, truncation=True, max_length=128)

        return image, text_tokens

def contrastive_loss(image_features, text_features, temperature=0.07):
    # Normalize embeddings
    image_features = nn.functional.normalize(image_features, p=2, dim=1)
    text_features = nn.functional.normalize(text_features, p=2, dim=1)

    # Similarity matrix
    logits = torch.matmul(image_features, text_features.T) / temperature

    # Targets: diagonal is the correct match
    targets = torch.arange(len(logits)).to(logits.device)

    # Cross-entropy loss
    loss = nn.CrossEntropyLoss()
    return loss(logits, targets) + loss(logits.T, targets)

import torch.optim as optim
from torchvision import transforms
from tqdm import tqdm

# Load tokenizer and create dataset
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
captions = ["A dog running in a field", "A person riding a bike", "A cat sitting on a couch"]  # Example captions
dataset = CustomDataset(image_dir="./images", captions=captions, tokenizer=tokenizer)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Initialize model, optimizer, and device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VLM().to(device)
optimizer = optim.AdamW(model.parameters(), lr=1e-4)

# Training loop
epochs = 5
for epoch in range(epochs):
    epoch_loss = 0
    model.train()

    for images, text_tokens in tqdm(dataloader):
        images = images.to(device)
        text_tokens = {key: val.squeeze(0).to(device) for key, val in text_tokens.items()}

        # Forward pass
        image_features, text_features = model(images, text_tokens)

        # Compute contrastive loss
        loss = contrastive_loss(image_features, text_features)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader)}")

torch.save(model.state_dict(), "vlm_pretrained.pth")

model.load_state_dict(torch.load("vlm_pretrained.pth"))
model.eval()

# Example inference
image, text_tokens = dataset[0]
image = image.unsqueeze(0).to(device)
text_tokens = {key: val.to(device) for key, val in text_tokens.items()}

with torch.no_grad():
    image_features, text_features = model(image, text_tokens)

print("Image and text embeddings:", image_features, text_features)